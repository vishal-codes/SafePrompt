{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "200e828a-e1ce-442c-803e-35c08e1eaed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a634b38a-4bf3-4706-8adb-ebc432950480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.4.0+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "VRAM free/total: 6.82 GB / 11.35 GB\n"
     ]
    }
   ],
   "source": [
    "# Clean slate: RAM/VRAM and deterministic runs\n",
    "import os, gc, random, torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Free Python + CUDA memory\n",
    "for name in list(globals().keys()):\n",
    "    if name not in [\"os\",\"gc\",\"random\",\"torch\",\"np\",\"numpy\",\"Path\"]:\n",
    "        del globals()[name]\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Helpful env toggles for Jupyter\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"   # avoid fork/threads deadlocks\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"  # we'll control AMP explicitly later\n",
    "\n",
    "# Project paths\n",
    "RUN_DIR = Path(\"runs/v2_safe_contract\")\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    dev = torch.cuda.get_device_name(0)\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    print(\"GPU:\", dev)\n",
    "    print(f\"VRAM free/total: {free/1e9:.2f} GB / {total/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "523eb39b-7bd7-4892-85d9-ea61c976e4ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set']\n",
      "Rows (English): 43501 → 43501 after dedupe\n",
      "Top 15 placeholders:\n",
      "             label  count\n",
      "0       FIRSTNAME   6155\n",
      "1        LASTNAME   2150\n",
      "2            DATE   2059\n",
      "3           EMAIL   1817\n",
      "4          PREFIX   1590\n",
      "5          AMOUNT   1483\n",
      "6        USERNAME   1379\n",
      "7        JOBTITLE   1358\n",
      "8             URL   1353\n",
      "9         JOBAREA   1326\n",
      "10    ACCOUNTNAME   1314\n",
      "11           TIME   1313\n",
      "12  ACCOUNTNUMBER   1307\n",
      "13     MIDDLENAME   1307\n",
      "14           CITY   1295\n",
      "Source tokens: n=20000, median=24, p90=36, p95=40\n",
      "Target tokens: n=20000, median=23, p90=34, p95=37\n",
      "Saved label vocab to: runs/v2_safe_contract/label_vocab_dataset.txt\n",
      "\n",
      "--- Sample 0 ---\n",
      "SRC: A student's assessment was found on device bearing IMEI: 06-184755-866851-3. The document falls under the various topics discussed in our Optimization curriculum. Can you please collect it?\n",
      "TGT: A student's assessment was found on device bearing IMEI: [PHONEIMEI]. The document falls under the various topics discussed in our [JOBAREA] curriculum. Can you please collect it?\n",
      "\n",
      "--- Sample 1 ---\n",
      "SRC: Dear Omer, as per our records, your license 78B5R2MVFAHJ48500 is still registered in our records for access to the educational tools. Please feedback on it's operability.\n",
      "TGT: Dear [FIRSTNAME], as per our records, your license [VEHICLEVIN] is still registered in our records for access to the educational tools. Please feedback on it's operability.\n",
      "\n",
      "--- Sample 2 ---\n",
      "SRC: Kattie could you please share your recomndations about vegetarian diet for 72 old Intersex person with 158centimeters?\n",
      "TGT: [FIRSTNAME] could you please share your recomndations about vegetarian diet for [AGE] old [GENDER] with [HEIGHT]?\n"
     ]
    }
   ],
   "source": [
    "# Load dataset + basic EDA (English only, placeholder vocab)\n",
    "from datasets import load_dataset, Dataset\n",
    "from collections import Counter\n",
    "import re, json, pandas as pd\n",
    "\n",
    "DATASET_ID = \"ai4privacy/pii-masking-200k\" \n",
    "\n",
    "ds = load_dataset(DATASET_ID)\n",
    "full = ds[\"train\"]\n",
    "\n",
    "print(\"Columns:\", full.column_names[:20])\n",
    "\n",
    "# Keep only english content\n",
    "full_en = full.filter(lambda x: str(x[\"language\"]).lower().startswith(\"en\"))\n",
    "\n",
    "# Keep only the columns we need\n",
    "needed_cols = [\"source_text\",\"target_text\"]\n",
    "missing = [c for c in needed_cols if c not in full_en.column_names]\n",
    "assert not missing, f\"Missing columns: {missing}\"\n",
    "\n",
    "def norm_space(t): \n",
    "    return re.sub(r\"\\s+\", \" \", t.strip()) if isinstance(t, str) else t\n",
    "    \n",
    "# add the normalised column\n",
    "full_en = full_en.map(lambda ex: {\"_src_norm\": norm_space(ex[\"source_text\"])})\n",
    "\n",
    "# convert -> drop -> back to HF Dataset\n",
    "df = full_en.to_pandas()\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=\"_src_norm\", keep=\"first\").drop(columns=[\"_src_norm\"])\n",
    "after = len(df)\n",
    "full_en = Dataset.from_pandas(df, preserve_index=False)\n",
    "print(f\"Rows (English): {before} -> {after} after dedupe\")\n",
    "\n",
    "# Build placeholder vocabulary from target_text: tokens like [FIRSTNAME]\n",
    "PH_RE = re.compile(r\"\\[([A-Za-z0-9_]+)\\]\")\n",
    "def extract_labels(txt):\n",
    "    return PH_RE.findall(txt or \"\")\n",
    "\n",
    "label_ctr = Counter()\n",
    "lengths_src, lengths_tgt = [], []\n",
    "\n",
    "for ex in full_en.select(range(min(20000, len(full_en)))):  # sample up to 20k for speed\n",
    "    for lab in extract_labels(ex[\"target_text\"]):\n",
    "        label_ctr[lab] += 1\n",
    "    lengths_src.append(len(ex[\"source_text\"].split()))\n",
    "    lengths_tgt.append(len(ex[\"target_text\"].split()))\n",
    "\n",
    "label_df = pd.DataFrame(\n",
    "    sorted(label_ctr.items(), key=lambda x: x[1], reverse=True),\n",
    "    columns=[\"label\",\"count\"]\n",
    ")\n",
    "print(\"Top 15 placeholders:\\n\", label_df.head(15))\n",
    "\n",
    "# Basic length stats to guide seq_len choice\n",
    "def pct(a, q): \n",
    "    i = int(len(a)*q); \n",
    "    return sorted(a)[i if i < len(a) else -1]\n",
    "print(f\"Source tokens: n={len(lengths_src)}, median={int(np.median(lengths_src))}, p90={pct(lengths_src,0.90)}, p95={pct(lengths_src,0.95)}\")\n",
    "print(f\"Target tokens: n={len(lengths_tgt)}, median={int(np.median(lengths_tgt))}, p90={pct(lengths_tgt,0.90)}, p95={pct(lengths_tgt,0.95)}\")\n",
    "\n",
    "# Save vocab for later formatting checks\n",
    "LABEL_VOCAB_PATH = RUN_DIR/\"label_vocab_dataset.txt\"\n",
    "with open(LABEL_VOCAB_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for lab, _ in sorted(label_ctr.items(), key=lambda x: (-x[1], x[0])):\n",
    "        f.write(lab + \"\\n\")\n",
    "print(\"Saved label vocab to:\", LABEL_VOCAB_PATH)\n",
    "\n",
    "# Peek a couple samples for sanity\n",
    "for i in [0,1,2]:\n",
    "    ex = full_en[i]\n",
    "    print(f\"\\n--- Sample {i} ---\")\n",
    "    print(\"SRC:\", ex[\"source_text\"][:300])\n",
    "    print(\"TGT:\", ex[\"target_text\"][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b937452-3f32-41ac-bbac-04af1288a1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 34800 4350 4351\n"
     ]
    }
   ],
   "source": [
    "# Step 2A split into train / val / test (80 / 10 / 10)\n",
    "from datasets import Dataset\n",
    "\n",
    "VAL_FRACTION  = 0.10\n",
    "TEST_FRACTION = 0.10\n",
    "SEED = 42\n",
    "\n",
    "# First carve out test\n",
    "split1 = full_en.train_test_split(test_size=TEST_FRACTION, seed=SEED)\n",
    "tmp_train, test_raw = split1[\"train\"], split1[\"test\"]\n",
    "\n",
    "# Now carve val from the remaining pool so final ratios are ~80/10/10\n",
    "val_size = VAL_FRACTION / (1.0 - TEST_FRACTION)  \n",
    "split2 = tmp_train.train_test_split(test_size=val_size, seed=SEED)\n",
    "train_raw, val_raw = split2[\"train\"], split2[\"test\"]\n",
    "\n",
    "print(\"Split sizes:\", len(train_raw), len(val_raw), len(test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a5195df-d686-4490-bda2-188f8137a9a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2cca0b0-6b02-4911-b46e-215e92529afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e60db2fcf0458b95cfea0c4eb963fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af204a339bdd40458c62783d13e7d42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259b3be14e23447c845668464d6f2f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/val/test (formatted): 34800 4350 4351\n",
      "\n",
      "Example formatted record (first ~1400 chars):\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a redactor. Return the EXACT input text with only PII spans replaced by dataset placeholders. Do NOT change any other words, punctuation, or casing. If unsure, keep. Wrap the final output inside <safe> and </safe>.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "The survey conducted on Intersex shows an alarming percentage experiencing early symptoms of PCOS. It's prevalent especially in the age group of 39 years old. Focusing on these demographic could be vital in future prevention.\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "<safe>The survey conducted on [GENDER] shows an alarming percentage experiencing early symptoms of PCOS. It's prevalent especially in the age group of [AGE]. Focusing on these demographic could be vital in future prevention.</safe>\n"
     ]
    }
   ],
   "source": [
    "# Step 2B format to chat with <safe>…</safe> as PLAIN STRINGS\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False, local_files_only=True)\n",
    "\n",
    "# pad token safety\n",
    "if tok.pad_token is None and tok.eos_token is not None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Llama 3.2 chat special tokens (strings)\n",
    "BOS = tok.bos_token or \"<|begin_of_text|>\"\n",
    "START = \"<|start_header_id|>\"\n",
    "END = \"<|end_header_id|>\"\n",
    "EOT = \"<|eot_id|>\"\n",
    "\n",
    "SAFE_OPEN, SAFE_CLOSE = \"<safe>\", \"</safe>\"\n",
    "\n",
    "SYSTEM_RULE = (\n",
    "    \"You are a redactor. Return the EXACT input text with only PII spans replaced by dataset placeholders. \"\n",
    "    \"Do NOT change any other words, punctuation, or casing. If unsure, keep. \"\n",
    "    \"Wrap the final output inside <safe> and </safe>.\"\n",
    ")\n",
    "\n",
    "def format_llama3_chat(system_text: str, user_text: str, assistant_text: str) -> str:\n",
    "    # Manual Llama-3 chat template as a STRING (no tokenization here)\n",
    "    return (\n",
    "        f\"{BOS}\"\n",
    "        f\"{START}system{END}\\n{system_text}\\n{EOT}\"\n",
    "        f\"{START}user{END}\\n{user_text}\\n{EOT}\"\n",
    "        f\"{START}assistant{END}\\n{assistant_text}\"\n",
    "    )\n",
    "\n",
    "def to_chat_text(ex):\n",
    "    assistant = f\"{SAFE_OPEN}{ex['target_text']}{SAFE_CLOSE}\"\n",
    "    text = format_llama3_chat(SYSTEM_RULE, ex[\"source_text\"], assistant)\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Rebuild formatted datasets as STRINGS\n",
    "train_ds = train_raw.map(to_chat_text, remove_columns=train_raw.column_names)\n",
    "val_ds   = val_raw.map(to_chat_text,   remove_columns=val_raw.column_names)\n",
    "test_ds  = test_raw.map(to_chat_text,  remove_columns=test_raw.column_names)\n",
    "\n",
    "print(\"train/val/test (formatted):\", len(train_ds), len(val_ds), len(test_ds))\n",
    "print(\"\\nExample formatted record (first ~1400 chars):\\n\")\n",
    "print(train_ds[0][\"text\"][:1400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d20395e-2648-4bd0-83e7-fb2702ab8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3A tag variants + helpers\n",
    "SAFE_OPEN  = \"<safe>\"\n",
    "SAFE_CLOSE = \"</safe>\"\n",
    "\n",
    "def make_tag_candidates(tag: str, tok):\n",
    "    variants = [tag, \" \" + tag, \"\\n\" + tag, \"\\r\\n\" + tag]\n",
    "    uniq = []\n",
    "    for v in variants:\n",
    "        ids = tok.encode(v, add_special_tokens=False)\n",
    "        if ids and ids not in uniq:\n",
    "            uniq.append(ids)\n",
    "    return uniq\n",
    "\n",
    "open_cands  = make_tag_candidates(SAFE_OPEN, tok)\n",
    "close_cands = make_tag_candidates(SAFE_CLOSE, tok)\n",
    "\n",
    "def find_subseq(hay, needle):\n",
    "    H, N = len(hay), len(needle)\n",
    "    if N == 0 or H < N: return -1\n",
    "    for i in range(H - N + 1):\n",
    "        if hay[i:i+N] == needle:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def find_any_tag(hay, candidates):\n",
    "    \"\"\"Return (pos, length) for the first matching candidate, else (-1, 0).\"\"\"\n",
    "    best_pos, best_len = -1, 0\n",
    "    for cand in candidates:\n",
    "        pos = find_subseq(hay, cand)\n",
    "        if pos != -1 and (best_pos == -1 or pos < best_pos):\n",
    "            best_pos, best_len = pos, len(cand)\n",
    "    return best_pos, best_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d414fd1-76d0-48c5-ab10-62342acf278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collator ready (precise prefix vs prefix+target).\n"
     ]
    }
   ],
   "source": [
    "# Step 3B Precise SafeCollator- map string span to token span via prefix vs (prefix+target)\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "SEQ_LEN = 512\n",
    "SAFE_OPEN, SAFE_CLOSE = \"<safe>\", \"</safe>\"\n",
    "\n",
    "class SafeCollator:\n",
    "    def __init__(self, tokenizer, max_len=SEQ_LEN):\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _ids(self, s: str):\n",
    "        return self.tok(\n",
    "            s,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        texts = [b[\"text\"] for b in batch]\n",
    "        enc = self.tok(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attn = enc[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            # Find char spans (use the LAST <safe> to avoid the mention in the system prompt)\n",
    "            open_idx = text.rfind(SAFE_OPEN)\n",
    "            if open_idx == -1:\n",
    "                labels[i, :] = -100\n",
    "                continue\n",
    "            start_idx = open_idx + len(SAFE_OPEN)\n",
    "            end_idx = text.find(SAFE_CLOSE, start_idx)\n",
    "            if end_idx == -1:\n",
    "                end_idx = len(text)\n",
    "\n",
    "            prefix = text[:start_idx]\n",
    "            target = text[start_idx:end_idx]\n",
    "\n",
    "            p_ids  = self._ids(prefix)\n",
    "            pt_ids = self._ids(prefix + target)\n",
    "\n",
    "            p_len  = len(p_ids)\n",
    "            pt_len = len(pt_ids)\n",
    "            L = int(attn[i].sum().item())  \n",
    "\n",
    "            if p_len >= L:\n",
    "                labels[i, :] = -100\n",
    "                continue\n",
    "\n",
    "            start_tok = p_len\n",
    "            end_tok   = min(L, pt_len) \n",
    "\n",
    "\\            labels[i, :start_tok] = -100\n",
    "            labels[i, end_tok:]   = -100\n",
    "\n",
    "        enc[\"labels\"] = labels\n",
    "        return enc\n",
    "\n",
    "collator = SafeCollator(tok, max_len=SEQ_LEN)\n",
    "print(\"Collator ready (precise prefix vs prefix+target).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82b8c1c2-e85e-41b1-a044-309b441922bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: (2, 156)\n",
      "labels    shape: (2, 156)\n",
      "\n",
      "--- Example 0 tokens_with_loss: 42\n",
      "Decoded text:   survey conducted on [GENDER] shows an alarming percentage experiencing early symptoms of PCOS. It's prevalent especially in the age group of [AGE]. Focusing on these demographic could be vital in future prevention.</\n",
      "\n",
      "--- Example 1 tokens_with_loss: 34\n",
      "Decoded text:  FIRSTNAME], the diagnostic device located at [NEARBYGPSCOORDINATE] identified you as [AGE]. The scan data confirmed the [EYECOLOR].\n"
     ]
    }
   ],
   "source": [
    "# Step 3C- sanity check the masking on a couple of samples\n",
    "batch = [train_ds[i] for i in range(2)]\n",
    "batch_out = collator(batch)\n",
    "\n",
    "print(\"input_ids shape:\", tuple(batch_out[\"input_ids\"].shape))\n",
    "print(\"labels    shape:\", tuple(batch_out[\"labels\"].shape))\n",
    "\n",
    "for i in range(len(batch)):\n",
    "    keep_mask = (batch_out[\"labels\"][i] != -100)\n",
    "    kept = batch_out[\"input_ids\"][i][keep_mask].tolist()\n",
    "    print(f\"\\n--- Example {i} tokens_with_loss:\", int(keep_mask.sum().item()))\n",
    "    print(\"Decoded text: \",tok.decode(kept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b5d7a02-173f-4139-bdfe-ca9c5e7c0c8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft) (2.4.0+cu121)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (from peft) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.11/site-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (1.2.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.82)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10f39221-3c34-4f3c-9030-35b70606c80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9004df05f748ba9b4e0ad5423ac03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 24,313,856 / 1,827,777,536 (1.330%)\n"
     ]
    }
   ],
   "source": [
    "# Step 4A - QLoRA model load\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "SEQ_LEN = 512\n",
    "\n",
    "# Safety free CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
    "\n",
    "# Tokenizer \n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False, local_files_only=True)\n",
    "if tok.pad_token is None and tok.eos_token is not None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# 4-bit quant config\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load base in 4-bit (use eager attention on 20-series GPUs)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_cfg,\n",
    "    attn_implementation=\"eager\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=False,\n",
    "    local_files_only=True,   # set False if you need to pull from hub\n",
    ")\n",
    "model_base.config.use_cache = False  # training\n",
    "\n",
    "# Prepare for k-bit training BEFORE adding LoRA\n",
    "model_base = prepare_model_for_kbit_training(model_base, use_gradient_checkpointing=False)\n",
    "\n",
    "# LoRA config - modest, stable\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model_base, lora_cfg)\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.3f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1c93b9d-9ddc-4ed7-8b27-cfe2c8574b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup subset size: 6000\n",
      "Starting v2 fast warmup…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 27:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.304800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 0.07227263100941976\n",
      "Saved adapter to: runs/v2_safe_contract/adapter_v2_fast\n"
     ]
    }
   ],
   "source": [
    "# Step 4B-mini: fast warmup\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# 1) Use a small random subset\n",
    "N_WARMUP = 6000\n",
    "train_ds_small = train_ds.shuffle(seed=42).select(range(min(N_WARMUP, len(train_ds))))\n",
    "print(\"Warmup subset size:\", len(train_ds_small))\n",
    "\n",
    "# 2) Shorten seq length and re-instantiate the collator\n",
    "SEQ_LEN = 320  # shorter context => faster; still far above p95 length for this dataset\n",
    "collator = SafeCollator(tok, max_len=SEQ_LEN)\n",
    "\n",
    "# 3) Training args tuned for speed\n",
    "args_fast = TrainingArguments(\n",
    "    output_dir=str(RUN_DIR/\"checkpoints_fast\"),\n",
    "    num_train_epochs=1,                 # single quick epoch\n",
    "    max_steps=0,                        # keep 0; OR set e.g. 400 to hard-cap runtime\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,     # effective batch = 16\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=True, bf16=False,              # if FP16 scaler complains, set fp16=False and optim=\"adamw_torch\"\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=False,\n",
    "    max_grad_norm=0.3,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    "    group_by_length=False,              # we tokenize in collator; keep this False\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,        # <-- important so 'text' reaches the collator\n",
    ")\n",
    "\n",
    "trainer_fast = Trainer(\n",
    "    model=model,                        # LoRA-wrapped model from 4A\n",
    "    args=args_fast,\n",
    "    train_dataset=train_ds_small,\n",
    "    eval_dataset=None,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tok,\n",
    ")\n",
    "\n",
    "print(\"Starting v2 fast warmup…\")\n",
    "res = trainer_fast.train()\n",
    "print(\"Final training loss:\", getattr(res, \"training_loss\", None))\n",
    "\n",
    "ADAPTER_DIR = RUN_DIR/\"adapter_v2_fast\"\n",
    "trainer_fast.model.save_pretrained(str(ADAPTER_DIR))\n",
    "tok.save_pretrained(str(ADAPTER_DIR))\n",
    "print(\"Saved adapter to:\", ADAPTER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60c8df95-bbbc-4a32-bea2-9efb07f1be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 1 ---\n",
      "IN : Hi, I am Vishal Shinde. Email me at vishal@example.com and call +1 415 555 0199.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT: Hi, I am [FIRSTNAME] [LASTNAME]. Email me at [EMAIL] and call [PHONENUMBER].\n",
      "\n",
      "--- Test 2 ---\n",
      "IN : Card 4111 1111 1111 1111 expires 12/26, CVV 123.\n",
      "OUT: Card [CREDITCARDCVV] [CREDITCARDNUMBER] expires [EXPIRYDATE], CVV [CREDITCARDCVV].\n",
      "\n",
      "--- Test 3 ---\n",
      "IN : Server at 10.0.0.5 uses API key sk_live_ABC123XYZ456 for uploads.\n",
      "OUT: Server at [IPV4] uses API key [CREDITCARDISSUER]_[CREDITCARDNUMBER] for uploads.\n",
      "\n",
      "--- Test 4 ---\n",
      "IN : IMEI: 06-184755-866851-3 belongs to this device.\n",
      "OUT: IMEI: [PHONEIMEI] belongs to this device.\n",
      "\n",
      "--- Test 5 ---\n",
      "IN : My SSN is 123-45-6789. Do not store it.\n",
      "OUT: My SSN is [SSN]. Do not store it.\n",
      "\n",
      "--- Test 6 ---\n",
      "IN : Hi Vishal, The phone with IMEI: 105879097227517  that you chose for trading in has passed our checks. You can send the package to us at 3000 Berkeley Avenue, AZ. If you have any questions, feel free to call us at +16328963421 or email us at phoneguy@gmail.com. Reply with not interested for the above email.\n",
      "OUT: Hi [FIRSTNAME], The phone with IMEI: [PHONEIMEI]  that you chose for trading in has passed our checks. You can send the package to us at [BUILDINGNUMBER] [STREET], [STATE]. If you have any questions, feel free to call us at [PHONENUMBER] or email us at [EMAIL]. Reply with not interested for the above email.\n"
     ]
    }
   ],
   "source": [
    "# 5A - helper + a few tests\n",
    "SAFE_OPEN, SAFE_CLOSE = \"<safe>\", \"</safe>\"\n",
    "BOS = tok.bos_token or \"<|begin_of_text|>\"\n",
    "START = \"<|start_header_id|>\"\n",
    "END = \"<|end_header_id|>\"\n",
    "EOT = \"<|eot_id|>\"\n",
    "\n",
    "SYSTEM_RULE = (\n",
    "    \"You are a redactor. Return the EXACT input text with only PII spans replaced by dataset placeholders. \"\n",
    "    \"Do NOT change any other words, punctuation, or casing. If unsure, keep. \"\n",
    "    \"Wrap the final output inside <safe> and </safe>.\"\n",
    ")\n",
    "\n",
    "def build_prompt(user_text: str) -> str:\n",
    "    return (\n",
    "        f\"{BOS}\"\n",
    "        f\"{START}system{END}\\n{SYSTEM_RULE}\\n{EOT}\"\n",
    "        f\"{START}user{END}\\n{user_text}\\n{EOT}\"\n",
    "        f\"{START}assistant{END}\\n{SAFE_OPEN}\"\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def redact_safe(user_text: str, max_new_tokens=96) -> str:\n",
    "    model.eval(); model.config.use_cache = True\n",
    "    prompt = build_prompt(user_text)\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "    )\n",
    "    decoded = tok.decode(out[0], skip_special_tokens=True)\n",
    "    # extract text between <safe> and </safe>\n",
    "    s = decoded.rfind(SAFE_OPEN)\n",
    "    if s != -1:\n",
    "        s += len(SAFE_OPEN)\n",
    "        e = decoded.find(SAFE_CLOSE, s)\n",
    "        piece = decoded[s:e if e != -1 else None]\n",
    "    else:\n",
    "        piece = decoded[len(prompt):]\n",
    "    return piece.strip()\n",
    "\n",
    "tests = [\n",
    "    \"Hi, I am Vishal Shinde. Email me at vishal@example.com and call +1 415 555 0199.\",\n",
    "    \"Card 4111 1111 1111 1111 expires 12/26, CVV 123.\",\n",
    "    \"Server at 10.0.0.5 uses API key sk_live_ABC123XYZ456 for uploads.\",\n",
    "    \"IMEI: 06-184755-866851-3 belongs to this device.\",\n",
    "    \"My SSN is 123-45-6789. Do not store it.\",\n",
    "    'Hi Vishal, The phone with IMEI: 105879097227517  that you chose for trading in has passed our checks. '\n",
    "    'You can send the package to us at 3000 Berkeley Avenue, AZ. If you have any questions, feel free to call us at '\n",
    "    '+16328963421 or email us at phoneguy@gmail.com. Reply with not interested for the above email.'\n",
    "]\n",
    "\n",
    "for i, t in enumerate(tests, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    print(\"IN :\", t)\n",
    "    print(\"OUT:\", redact_safe(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4afc340-2b03-4a1f-800e-cc1aaec93c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluated 300 examples\n",
      "Exact match rate: 0.670\n",
      "Placeholder micro-F1: P=0.911 R=0.896 F1=0.903\n",
      "Formatting error rate: 0.000\n"
     ]
    }
   ],
   "source": [
    "# 5B - batched eval: exact match, placeholder micro-F1, formatting errors\n",
    "PLACEHOLDER_RE = re.compile(r\"\\[([A-Za-z0-9_]+)\\]\")\n",
    "\n",
    "def extract_labels(text): \n",
    "    return [m.group(1) for m in PLACEHOLDER_RE.finditer(text or \"\")]\n",
    "\n",
    "def count_map(labels):\n",
    "    c = Counter()\n",
    "    for l in labels: c[l]+=1\n",
    "    return c\n",
    "\n",
    "# Allowed placeholders\n",
    "ALLOWED = set([ln.strip() for ln in open(Path(\"runs/v2_safe_contract\")/\"label_vocab_dataset.txt\", \"r\", encoding=\"utf-8\") if ln.strip()])\n",
    "\n",
    "def formatting_errors(text):\n",
    "    errs = []\n",
    "    if (text or \"\").count(\"[\") != (text or \"\").count(\"]\"):\n",
    "        errs.append(\"bracket_mismatch\")\n",
    "    unk = sorted(set(l for l in extract_labels(text) if l not in ALLOWED))\n",
    "    if unk: errs.append(\"unknown:\" + \"|\".join(unk))\n",
    "    return errs\n",
    "\n",
    "def prf(tp, fp, fn):\n",
    "    P = tp/(tp+fp) if (tp+fp) else 0.0\n",
    "    R = tp/(tp+fn) if (tp+fn) else 0.0\n",
    "    F = 2*P*R/(P+R) if (P+R) else 0.0\n",
    "    return P, R, F\n",
    "\n",
    "N_EVAL = 300\n",
    "BATCH  = 4\n",
    "MAX_NEW = 96\n",
    "\n",
    "rows = test_raw.shuffle(seed=42).select(range(min(N_EVAL, len(test_raw))))\n",
    "srcs  = [r[\"source_text\"] for r in rows]\n",
    "golds = [r[\"target_text\"] for r in rows]\n",
    "\n",
    "preds=[]\n",
    "for i in range(0, len(srcs), BATCH):\n",
    "    batch = srcs[i:i+BATCH]\n",
    "    for s in batch:\n",
    "        preds.append(redact_safe(s, max_new_tokens=MAX_NEW))\n",
    "\n",
    "# metrics\n",
    "exact, fmt_err = 0, 0\n",
    "tp=Counter(); fp=Counter(); fn=Counter()\n",
    "\n",
    "for pred, gold in zip(preds, golds):\n",
    "    if pred == gold: exact += 1\n",
    "    if formatting_errors(pred): fmt_err += 1\n",
    "    pC = count_map(extract_labels(pred))\n",
    "    gC = count_map(extract_labels(gold))\n",
    "    labs = set(pC)|set(gC)\n",
    "    for l in labs:\n",
    "        tp[l] += min(pC.get(l,0), gC.get(l,0))\n",
    "        fp[l] += max(pC.get(l,0)-gC.get(l,0), 0)\n",
    "        fn[l] += max(gC.get(l,0)-pC.get(l,0), 0)\n",
    "\n",
    "P_micro, R_micro, F_micro = prf(sum(tp.values()), sum(fp.values()), sum(fn.values()))\n",
    "print(f\"\\nEvaluated {len(preds)} examples\")\n",
    "print(f\"Exact match rate: {exact/len(preds):.3f}\")\n",
    "print(f\"Placeholder micro-F1: P={P_micro:.3f} R={R_micro:.3f} F1={F_micro:.3f}\")\n",
    "print(f\"Formatting error rate: {fmt_err/len(preds):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87b05522-c026-422d-8036-b5dc8d3b0b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 1 ---\n",
      "IN : Hi, I am Vishal Shinde. Email me at vishal@example.com and call +1 415 555 0199.\n",
      "OUT: Hi, I am [FIRSTNAME] [LASTNAME]. Email me at [EMAIL] and call [PHONENUMBER].\n",
      "\n",
      "--- Test 2 ---\n",
      "IN : Card 4111 1111 1111 1111 expires 12/26, CVV 123.\n",
      "OUT: Card [CREDITCARDCVV] [CREDITCARDNUMBER] expires [EXPIRYDATE], CVV [CREDITCARDCVV].\n",
      "\n",
      "--- Test 3 ---\n",
      "IN : Server at 10.0.0.5 uses API key sk_live_ABC123XYZ456 for uploads.\n",
      "OUT: Server at [IPV4] uses API key [CREDITCARDISSUER]_[CREDITCARDNUMBER] for uploads.\n",
      "\n",
      "--- Test 4 ---\n",
      "IN : IMEI: 06-184755-866851-3 belongs to this device.\n",
      "OUT: IMEI: [PHONEIMEI] belongs to this device.\n",
      "\n",
      "--- Test 5 ---\n",
      "IN : My SSN is 123-45-6789. Do not store it.\n",
      "OUT: My SSN is [SSN]. Do not store it.\n",
      "\n",
      "--- Test 6 ---\n",
      "IN : Hi Vishal, The phone with IMEI: 105879097227517  that you chose for trading in has passed our checks. You can send the package to us at 3000 Berkeley Avenue, AZ. If you have any questions, feel free to call us at +16328963421 or email us at phoneguy@gmail.com. Reply with not interested for the above email.\n",
      "OUT: Hi [FIRSTNAME], The phone with IMEI: [PHONEIMEI]  that you chose for trading in has passed our checks. You can send the package to us at [BUILDINGNUMBER] [STREET], [STATE]. If you have any questions, feel free to call us at [PHONENUMBER] or email us at [EMAIL]. Reply with not interested for the above email.\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"Hi, I am Vishal Shinde. Email me at vishal@example.com and call +1 415 555 0199.\",\n",
    "    \"Card 4111 1111 1111 1111 expires 12/26, CVV 123.\",\n",
    "    \"Server at 10.0.0.5 uses API key sk_live_ABC123XYZ456 for uploads.\",\n",
    "    \"IMEI: 06-184755-866851-3 belongs to this device.\",\n",
    "    \"My SSN is 123-45-6789. Do not store it.\",\n",
    "    'Hi Vishal, The phone with IMEI: 105879097227517  that you chose for trading in has passed our checks. '\n",
    "    'You can send the package to us at 3000 Berkeley Avenue, AZ. If you have any questions, feel free to call us at '\n",
    "    '+16328963421 or email us at phoneguy@gmail.com. Reply with not interested for the above email.'\n",
    "]\n",
    "\n",
    "for i, t in enumerate(tests, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    print(\"IN :\", t)\n",
    "    print(\"OUT:\", redact_safe(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8b6ed-cfe8-431f-8019-cc10ca0976c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644fcd9e-6df0-4016-b998-602c24146400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56cef8-df8d-4b68-bffb-16784c88bf11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f34efc-e9df-4eb8-a729-4b086f5b7810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
