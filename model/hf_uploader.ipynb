{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e00ae2-42f4-4c63-b17a-c4579f6e90e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No HF token in env. You will be prompted. Create one at https://huggingface.co/settings/tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7971475a1040189fe668c863310dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: chinu-codes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "# Use an env var if you have set one, else you will be prompted in the notebook.\n",
    "token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if token:\n",
    "    login(token=token, add_to_git_credential=True)\n",
    "else:\n",
    "    print(\"No HF token in env. You will be prompted. Create one at https://huggingface.co/settings/tokens\")\n",
    "    login(add_to_git_credential=True)\n",
    "\n",
    "print(\"Authenticated as:\", whoami().get(\"name\") or whoami().get(\"email\") or \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb45d98-6d9c-4ccd-b1e2-c9513261193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter directory exists: ./outputs/safe-prompt-3b-lora\n",
      "Target repo: chinu-codes/safe-prompt-llama-3_2-3b-lora\n",
      "Repo ready: https://huggingface.co/chinu-codes/safe-prompt-llama-3_2-3b-lora\n"
     ]
    }
   ],
   "source": [
    "# Step 1.1 — Create a new public HF model repo for your adapters\n",
    "from huggingface_hub import whoami, create_repo\n",
    "import os, pathlib\n",
    "\n",
    "# Config — change the name if you prefer\n",
    "REPO_NAME   = \"safe-prompt-llama-3_2-3b-lora\"   # e.g. \"safe-prompt-v1\"\n",
    "ADAPTER_DIR = \"./outputs/safe-prompt-3b-lora\"   # where your adapters were saved\n",
    "\n",
    "# Sanity checks\n",
    "assert pathlib.Path(ADAPTER_DIR).exists(), f\"Adapter dir not found: {ADAPTER_DIR}\"\n",
    "print(\"Adapter directory exists:\", ADAPTER_DIR)\n",
    "\n",
    "# Resolve your namespace\n",
    "me = whoami()\n",
    "namespace = me.get(\"name\") or (me.get(\"orgs\") or [None])[0]\n",
    "assert namespace, \"Could not resolve your HF namespace. Are you logged in?\"\n",
    "\n",
    "repo_id = f\"{namespace}/{REPO_NAME}\"\n",
    "print(\"Target repo:\", repo_id)\n",
    "\n",
    "# Create (idempotent)\n",
    "repo_url = create_repo(repo_id=repo_id, repo_type=\"model\", private=False, exist_ok=True)\n",
    "print(\"Repo ready:\", repo_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f88ac6-99e0-4915-b641-87e303817fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated handler with pluggable validators at: ./outputs/safe-prompt-3b-lora/handler.py\n"
     ]
    }
   ],
   "source": [
    "# Overwrite ./outputs/safe-prompt-3b-lora/handler.py to include pluggable validators\n",
    "import os, pathlib, textwrap\n",
    "\n",
    "ADAPTER_DIR = \"./outputs/safe-prompt-3b-lora\"\n",
    "pathlib.Path(ADAPTER_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "handler_py = \"\"\"\n",
    "import os, re, json, torch\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "SAFE_OPEN  = \"<safe>\"\n",
    "SAFE_CLOSE = \"</safe>\"\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You redact personal or secret information from user text. \"\n",
    "    \"Return the SAME text but replace only the sensitive VALUES with placeholders. \"\n",
    "    \"Do not change surrounding words like 'IMEI', 'Email', 'Phone', or punctuation. \"\n",
    "    \"Allowed placeholders include dataset-style tags like [EMAIL], [PHONEIMEI], [FIRSTNAME], etc. \"\n",
    "    \"Output ONLY the redacted text between <safe> and </safe>. No other text.\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Pluggable validators (seatbelts)\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class Detector:\n",
    "    name: str\n",
    "    pattern: re.Pattern\n",
    "    placeholder: str\n",
    "\n",
    "DETECTORS: List[Detector] = [\n",
    "    Detector(\"email\", re.compile(r\"\\\\b[^\\\\s@]+@[^\\\\s@]+\\\\.[^\\\\s@]+\\\\b\"), \"[EMAIL]\"),\n",
    "    Detector(\"phone\", re.compile(r\"\\\\b(?:\\\\+?\\\\d{1,3}[\\\\s.\\\\-]?)?(?:\\\\(?\\\\d{3}\\\\)?[\\\\s.\\\\-]?)?\\\\d{3}[\\\\s.\\\\-]?\\\\d{4}\\\\b\"), \"[PHONE]\"),\n",
    "    # Add more when needed, e.g.:\n",
    "    # Detector(\"ipv4\", re.compile(r\"\\\\b(?:(?:25[0-5]|2[0-4]\\\\d|[01]?\\\\d\\\\d?)\\\\.){3}(?:25[0-5]|2[0-4]\\\\d|[01]?\\\\d\\\\d?)\\\\b\"), \"[IP]\"),\n",
    "    # Detector(\"ssn_us\", re.compile(r\"\\\\b\\\\d{3}-\\\\d{2}-\\\\d{4}\\\\b\"), \"[SSN]\"),\n",
    "]\n",
    "\n",
    "def apply_validators(text: str, mode: str = \"enforce\") -> Tuple[str, List[str]]:\n",
    "    \\\"\\\"\\\"mode = off | warn | enforce\\\"\\\"\\\"\n",
    "    hits: List[str] = []\n",
    "    out = text\n",
    "    for d in DETECTORS:\n",
    "        if d.pattern.search(out):\n",
    "            hits.append(d.name)\n",
    "            if mode == \"enforce\":\n",
    "                out = d.pattern.sub(d.placeholder, out)\n",
    "    # In 'warn' mode we do not modify output, just record hits\n",
    "    return out, hits\n",
    "\n",
    "# --------------------------\n",
    "# Endpoint handler\n",
    "# --------------------------\n",
    "class EndpointHandler:\n",
    "    def __init__(self, path: str = \"\"):\n",
    "        # 'path' is the mounted repo dir (contains adapters + adapter_config.json)\n",
    "        self.repo_path = path\n",
    "        self.seq_len   = int(os.getenv(\"SEQ_LEN\", \"512\"))\n",
    "        self.max_new   = int(os.getenv(\"MAX_NEW_TOKENS\", \"128\"))\n",
    "        self.dtype     = torch.float16\n",
    "        self.validate_mode = os.getenv(\"VALIDATE_MODE\", \"enforce\").lower()  # off|warn|enforce\n",
    "\n",
    "        # Read base model id from adapter_config.json if present\n",
    "        adapter_cfg = os.path.join(path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_cfg):\n",
    "            with open(adapter_cfg, \"r\", encoding=\"utf-8\") as f:\n",
    "                cfg = json.load(f)\n",
    "            self.base_model_id = cfg.get(\"base_model_name_or_path\", \"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "        else:\n",
    "            self.base_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_id, use_fast=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        self.tokenizer.model_max_length = self.seq_len\n",
    "\n",
    "        # Model (try 4-bit, fall back to fp16)\n",
    "        kwargs = dict(torch_dtype=self.dtype, device_map=\"auto\")\n",
    "        try:\n",
    "            bnb_cfg = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=self.dtype,\n",
    "            )\n",
    "            kwargs[\"quantization_config\"] = bnb_cfg\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        self.model = AutoPeftModelForCausalLM.from_pretrained(path, **kwargs)\n",
    "        self.model.config.use_cache = False\n",
    "        self.model.eval()\n",
    "\n",
    "        self.gen = pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def _make_prompt(self, text: str) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "            {\"role\": \"assistant\", \"content\": SAFE_OPEN},\n",
    "        ]\n",
    "        try:\n",
    "            return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        except Exception:\n",
    "            return f\"<|system|>\\\\n{SYSTEM}\\\\n<|user|>\\\\n{text}\\\\n<|assistant|>\\\\n{SAFE_OPEN}\"\n",
    "\n",
    "    def _strip_tags(self, s: str) -> str:\n",
    "        # Remove any accidental nested or echoed tags\n",
    "        return s.replace(SAFE_OPEN, \"\").replace(SAFE_CLOSE, \"\").strip()\n",
    "\n",
    "    def __call__(self, data: Any) -> str:\n",
    "        # Accept { \"inputs\": \"...\" } or raw str/list[str]\n",
    "        text = None\n",
    "        if isinstance(data, dict):\n",
    "            text = data.get(\"inputs\")\n",
    "        elif isinstance(data, str):\n",
    "            text = data\n",
    "        elif isinstance(data, list) and data and isinstance(data[0], str):\n",
    "            text = data[0]\n",
    "\n",
    "        if not text:\n",
    "            return f\"{SAFE_OPEN}{SAFE_CLOSE}\"\n",
    "\n",
    "        prompt = self._make_prompt(text)\n",
    "        out = self.gen(\n",
    "            prompt,\n",
    "            max_new_tokens=self.max_new,\n",
    "            do_sample=False,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            return_full_text=False,\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        # Trim at first </safe>, strip tags, then validators\n",
    "        if SAFE_CLOSE in out:\n",
    "            out = out.split(SAFE_CLOSE, 1)[0]\n",
    "        out = self._strip_tags(out)\n",
    "\n",
    "        out, hits = apply_validators(out, mode=self.validate_mode)\n",
    "        # Optional: print hits in warn mode to logs without changing output\n",
    "        if hits and self.validate_mode == \"warn\":\n",
    "            print(f\"[validator hits] {hits}\")\n",
    "\n",
    "        return f\"{SAFE_OPEN}{out}{SAFE_CLOSE}\"\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(ADAPTER_DIR, \"handler.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(handler_py)\n",
    "\n",
    "print(\"Updated handler with pluggable validators at:\", os.path.join(ADAPTER_DIR, \"handler.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a8cf3f-067e-4bfb-867c-aa221d66dd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ./outputs/safe-prompt-3b-lora/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Step 1.3 — Write requirements.txt for the endpoint container\n",
    "import os, pathlib, textwrap\n",
    "\n",
    "ADAPTER_DIR = \"./outputs/safe-prompt-3b-lora\"\n",
    "pathlib.Path(ADAPTER_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "reqs = textwrap.dedent(\"\"\"\n",
    "transformers==4.57.1\n",
    "peft==0.17.1\n",
    "bitsandbytes==0.48.1\n",
    "accelerate==1.11.0\n",
    "safetensors>=0.4.5\n",
    "\"\"\").strip() + \"\\n\"\n",
    "\n",
    "with open(os.path.join(ADAPTER_DIR, \"requirements.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(reqs)\n",
    "\n",
    "print(\"Wrote:\", os.path.join(ADAPTER_DIR, \"requirements.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a131f1-62f8-49df-b557-915365a2ac2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading folder to: chinu-codes/safe-prompt-llama-3_2-3b-lora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/hf_api.py:9662: UserWarning: Warnings while validating metadata in README.md:\n",
      "- empty or missing yaml metadata in repo card\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14c889d66ff4d319fc095070426a1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7a9bb7b0874caca84f89e21034ffa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload complete.\n",
      "\n",
      "Files in repo:\n",
      "- .gitattributes\n",
      "- README.md\n",
      "- adapter_config.json\n",
      "- adapter_model.safetensors\n",
      "- chat_template.jinja\n",
      "- checkpoint-113/README.md\n",
      "- checkpoint-113/adapter_config.json\n",
      "- checkpoint-113/adapter_model.safetensors\n",
      "- checkpoint-113/chat_template.jinja\n",
      "- checkpoint-113/optimizer.pt\n",
      "- checkpoint-113/rng_state.pth\n",
      "- checkpoint-113/scaler.pt\n",
      "- checkpoint-113/scheduler.pt\n",
      "- checkpoint-113/special_tokens_map.json\n",
      "- checkpoint-113/tokenizer.json\n",
      "- checkpoint-113/tokenizer_config.json\n",
      "- checkpoint-113/trainer_state.json\n",
      "- checkpoint-113/training_args.bin\n",
      "- handler.py\n",
      "- requirements.txt\n",
      "- special_tokens_map.json\n",
      "- tokenizer.json\n",
      "- tokenizer_config.json\n",
      "- training_args.bin\n"
     ]
    }
   ],
   "source": [
    "# Step 1.5 — Upload adapters + handler + requirements + README to Hugging Face\n",
    "import os, pathlib\n",
    "from huggingface_hub import HfApi, upload_folder, whoami\n",
    "\n",
    "ADAPTER_DIR = \"./outputs/safe-prompt-3b-lora\"\n",
    "REPO_ID     = \"chinu-codes/safe-prompt-llama-3_2-3b-lora\"\n",
    "\n",
    "# Sanity checks\n",
    "assert pathlib.Path(ADAPTER_DIR).exists(), f\"Adapter dir not found: {ADAPTER_DIR}\"\n",
    "for must in [\"handler.py\", \"requirements.txt\", \"README.md\"]:\n",
    "    p = pathlib.Path(ADAPTER_DIR) / must\n",
    "    assert p.exists(), f\"Missing file: {p}\"\n",
    "\n",
    "print(\"Uploading folder to:\", REPO_ID)\n",
    "upload_folder(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"model\",\n",
    "    folder_path=ADAPTER_DIR,\n",
    "    commit_message=\"Add PEFT adapters + custom handler + requirements + README\",\n",
    "    ignore_patterns=[\"*.ipynb_checkpoints*\", \"*.DS_Store\"],\n",
    ")\n",
    "print(\"Upload complete.\")\n",
    "\n",
    "# List files to verify\n",
    "api = HfApi()\n",
    "files = api.list_repo_files(REPO_ID, repo_type=\"model\")\n",
    "print(\"\\nFiles in repo:\")\n",
    "for f in files:\n",
    "    print(\"-\", f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
