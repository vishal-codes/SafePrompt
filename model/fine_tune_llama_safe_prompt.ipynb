{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1257bfac-2dfe-45f6-81d9-e5df8a0f5aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if not already available\n",
    "!pip install gputil psutil -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe9fdcb-842f-45b1-8ba3-dffa006ab669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ðŸ§  CPU INFORMATION ===\n",
      "System: Linux 6.8.0-85-generic\n",
      "Processor: x86_64\n",
      "Architecture: x86_64\n",
      "CPU Cores: 32 (Physical), 64 (Logical)\n",
      "Total RAM: 251.75 GB\n",
      "\n",
      "=== âš¡ GPU INFORMATION ===\n",
      "GPU ID: 0\n",
      "Name: NVIDIA GeForce GTX 1080 Ti\n",
      "Driver Version: 580.95.05\n",
      "Total Memory: 11264.0 MB\n",
      "Used Memory: 3.0 MB\n",
      "Free Memory: 11164.0 MB\n",
      "GPU Load: 0.0%\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "import platform\n",
    "import psutil\n",
    "\n",
    "def show_hardware_info():\n",
    "    print(\"=== ðŸ§  CPU INFORMATION ===\")\n",
    "    print(f\"System: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Processor: {platform.processor()}\")\n",
    "    print(f\"Architecture: {platform.machine()}\")\n",
    "    print(f\"CPU Cores: {psutil.cpu_count(logical=False)} (Physical), {psutil.cpu_count(logical=True)} (Logical)\")\n",
    "    print(f\"Total RAM: {round(psutil.virtual_memory().total / (1024**3), 2)} GB\")\n",
    "\n",
    "    print(\"\\n=== âš¡ GPU INFORMATION ===\")\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    if not gpus:\n",
    "        print(\"No GPU detected.\")\n",
    "    else:\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU ID: {gpu.id}\")\n",
    "            print(f\"Name: {gpu.name}\")\n",
    "            print(f\"Driver Version: {gpu.driver}\")\n",
    "            print(f\"Total Memory: {gpu.memoryTotal} MB\")\n",
    "            print(f\"Used Memory: {gpu.memoryUsed} MB\")\n",
    "            print(f\"Free Memory: {gpu.memoryFree} MB\")\n",
    "            print(f\"GPU Load: {gpu.load * 100:.1f}%\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "show_hardware_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e266b9-9977-4268-878c-03280582c006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers>=4.45.0\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m258.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.34.0\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft>=0.12.0\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting datasets>=2.20.0\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting trl>=0.9.6\n",
      "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting bitsandbytes>=0.44.1\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers>=4.45.0) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.45.0)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.45.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.45.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.45.0) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.45.0)\n",
      "  Downloading regex-2025.10.23-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m244.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers>=4.45.0) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.45.0)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.45.0)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.45.0) (4.66.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.34.0) (6.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.34.0) (2.4.0+cu121)\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=2.20.0)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.20.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets>=2.20.0) (2.2.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.20.0) (0.27.0)\n",
      "Collecting xxhash (from datasets>=2.20.0)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.20.0)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20.0) (2024.6.1)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20.0)\n",
      "  Downloading aiohttp-3.13.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.20.0) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.20.0) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.20.0) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.20.0) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.20.0) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.20.0) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.45.0) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers>=4.45.0)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.45.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.45.0) (2.2.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate>=0.34.0) (12.5.82)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=2.20.0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=2.20.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=2.20.0) (2024.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20.0)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20.0)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20.0) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20.0)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20.0)\n",
      "  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20.0)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20.0)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m512.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.0.0->accelerate>=0.34.0) (1.3.0)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m949.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.10.23-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m800.3/800.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m246.7/246.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m365.8/365.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, safetensors, regex, pyarrow, propcache, multiprocess, multidict, hf-xet, frozenlist, aiohappyeyeballs, yarl, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, bitsandbytes, accelerate, peft, datasets, trl\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 17.0.0\n",
      "    Uninstalling pyarrow-17.0.0:\n",
      "      Successfully uninstalled pyarrow-17.0.0\n",
      "Successfully installed accelerate-1.11.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.1 aiosignal-1.4.0 bitsandbytes-0.48.1 datasets-4.3.0 frozenlist-1.8.0 hf-xet-1.2.0 huggingface-hub-0.36.0 multidict-6.7.0 multiprocess-0.70.16 peft-0.17.1 propcache-0.4.1 pyarrow-22.0.0 regex-2025.10.23 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.57.1 trl-0.24.0 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "# If you already have these, skip. Pin to stable versions.\n",
    "!pip install \"transformers>=4.45.0\" \"accelerate>=0.34.0\" \"peft>=0.12.0\" \"datasets>=2.20.0\" \"trl>=0.9.6\" \"bitsandbytes>=0.44.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "999c00f2-efd5-4bfd-b507-30e5e22d2a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.4.0+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce GTX 1080 Ti | SM capability: (6, 1) | Total VRAM: 10.90 GB\n",
      "\n",
      "=== CONFIG SUMMARY ===\n",
      "MODEL_ID: meta-llama/Llama-3.2-3B-Instruct\n",
      "SEQ_LEN: 512\n",
      "DTYPE: torch.float16\n",
      "BNB imported: True\n",
      "ALLOW_4BIT: True\n",
      "USE_4BIT (requested): True\n",
      "LoRA r/alpha/dropout: 16 32 0.05\n",
      "Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1) Basic environment info\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    dev = torch.cuda.current_device()\n",
    "    name = torch.cuda.get_device_name(dev)\n",
    "    cap  = torch.cuda.get_device_capability(dev)\n",
    "    print(f\"GPU: {name} | SM capability: {cap} | Total VRAM: {torch.cuda.get_device_properties(dev).total_memory/1024**3:.2f} GB\")\n",
    "\n",
    "# 2) BitsAndBytes availability check\n",
    "try:\n",
    "    import bitsandbytes as bnb  # noqa\n",
    "    BNB_IMPORTED = True\n",
    "except Exception as e:\n",
    "    BNB_IMPORTED = False\n",
    "    print(\"bitsandbytes import failed:\", repr(e))\n",
    "\n",
    "sm_major, sm_minor = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
    "ALLOW_4BIT = bool(torch.cuda.is_available() and BNB_IMPORTED and sm_major >= 6)\n",
    "\n",
    "# 3) Model choice and conservative defaults for your 1080 Ti (11 GB)\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"   # we will load later\n",
    "SEQ_LEN = 512\n",
    "DTYPE = torch.float16        # Pascal cards do not support bfloat16\n",
    "USE_4BIT = True              # we will fall back if ALLOW_4BIT is False\n",
    "GRAD_ACCUM = 16\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "print(\"\\n=== CONFIG SUMMARY ===\")\n",
    "print(\"MODEL_ID:\", MODEL_ID)\n",
    "print(\"SEQ_LEN:\", SEQ_LEN)\n",
    "print(\"DTYPE:\", DTYPE)\n",
    "print(\"BNB imported:\", BNB_IMPORTED)\n",
    "print(\"ALLOW_4BIT:\", ALLOW_4BIT)\n",
    "print(\"USE_4BIT (requested):\", USE_4BIT)\n",
    "print(\"LoRA r/alpha/dropout:\", LORA_R, LORA_ALPHA, LORA_DROPOUT)\n",
    "print(\"Target modules:\", TARGET_MODULES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc308e48-e9ff-46e4-a482-d8106c9b2858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No HF token in env. You will be prompted. Create one at https://huggingface.co/settings/tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4798e90df2304881b3135d9084d12993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: chinu-codes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5792e87104498ebf705f52a8f79279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934374006936416fa69f878376c35300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "english_pii_43k.jsonl:   0%|          | 0.00/73.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84094c4921774f9a8b83ae1ba0fb5380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "french_pii_62k.jsonl:   0%|          | 0.00/116M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510a3a1b01aa48f7b12b8020904951b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "german_pii_52k.jsonl:   0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10e3c68b4e24d94996b7b0647e9bad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "italian_pii_50k.jsonl:   0%|          | 0.00/93.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f1e2be761747d9a91a46907d33a431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/209261 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set'],\n",
      "    num_rows: 200\n",
      "})\n",
      "Example keys: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set']\n",
      "- source_text: A student's assessment was found on device bearing IMEI: 06-184755-866851-3. The document falls under the various topics discussed in our Optimization curriculum. Can you please collect it?\n",
      "- target_text: A student's assessment was found on device bearing IMEI: [PHONEIMEI]. The document falls under the various topics discussed in our [JOBAREA] curriculum. Can you please collect it?\n",
      "- privacy_mask: [{'value': '06-184755-866851-3', 'start': 57, 'end': 75, 'label': 'PHONEIMEI'}, {'value': 'Optimization', 'start': 138, 'end': 150, 'label': 'JOBAREA'}]\n",
      "- span_labels: [[0, 57, \"O\"], [57, 75, \"PHONEIMEI\"], [75, 138, \"O\"], [138, 150, \"JOBAREA\"], [150, 189, \"O\"]]\n",
      "- mbert_text_tokens: ['A', 'student', \"'\", 's', 'assessment', 'was', 'found', 'on', 'device', 'bearing', 'IM', '##E', '##I', ':', '06', '-', '1847', '##55', '-', '866', '##85', '##1', '-', '3', '.', 'The', 'document', 'fa...\n",
      "- mbert_bio_labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONEIMEI', 'I-PHONEIMEI', 'I-PHONEIMEI', 'I-PHONEIMEI', 'I-PHONEIMEI', 'I-PHONEIMEI', 'I-PHONEIMEI', 'I-PHONEIMEI', 'I-PHONEI...\n",
      "- id: 165761\n",
      "- language: en\n",
      "- set: train\n"
     ]
    }
   ],
   "source": [
    "# Step 3 â€” HF login and dataset smoke test\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"  # keep things quiet\n",
    "\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "# Use an env var if you have set one, else you will be prompted in the notebook.\n",
    "token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if token:\n",
    "    login(token=token, add_to_git_credential=True)\n",
    "else:\n",
    "    print(\"No HF token in env. You will be prompted. Create one at https://huggingface.co/settings/tokens\")\n",
    "    login(add_to_git_credential=True)\n",
    "\n",
    "print(\"Authenticated as:\", whoami().get(\"name\") or whoami().get(\"email\") or \"unknown\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_ID = \"ai4privacy/pii-masking-200k\"\n",
    "\n",
    "# Load a tiny slice to keep RAM light for now\n",
    "ds = load_dataset(DATASET_ID, split=\"train[:200]\")\n",
    "print(ds)\n",
    "\n",
    "# Peek at one record so we know the field names\n",
    "ex = ds[0]\n",
    "print(\"Example keys:\", list(ex.keys()))\n",
    "for k, v in ex.items():\n",
    "    s = str(v)\n",
    "    print(f\"- {k}: {s[:200]}{'...' if len(s) > 200 else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4d021be-a7ff-47bf-a2ab-97a8d15b4b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: A student's assessment was found on device bearing IMEI: 06-184755-866851-3. The document falls under the various topics discussed in our Optimization curriculum. Can you please collect it?\n",
      "\n",
      "PREDICTED:\n",
      "<safe>A student's assessment was found on device bearing [PHONEIMEI]. The document falls under the various topics discussed in our Optimization curriculum. Can you please collect it?</safe>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "# Reuse 'tokenizer' and 'base_model' already loaded\n",
    "\n",
    "SYSTEM_V2 = (\n",
    "    \"You redact personal or secret information from user text. \"\n",
    "    \"Return the SAME text but with only the sensitive VALUES replaced by placeholders. \"\n",
    "    \"Do NOT change surrounding words like 'IMEI', 'Email', 'Phone', labels, or punctuation. \"\n",
    "    \"Allowed placeholders: [NAME_1], [EMAIL_1], [PHONE_1], [ADDRESS_1], [DOB_1], [SSN_1], [CARD_1], \"\n",
    "    \"[IP_1], [USERNAME_1], [PASSWORD_1], [APIKEY_1], and dataset-style tags like [PHONEIMEI]. \"\n",
    "    \"Output ONLY the redacted text between <safe> and </safe>. No other text.\"\n",
    ")\n",
    "\n",
    "def make_prompt_v2(text: str) -> str:\n",
    "    # No <<< >>> delimiters; keep it minimal and deterministic\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_V2},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "        {\"role\": \"assistant\", \"content\": \"<safe>\"}  # bias start of the block\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "SAFE_BLOCK_RE = re.compile(r\"<safe>(.*?)</safe>\", flags=re.DOTALL)\n",
    "\n",
    "def extract_safe(generated: str) -> str:\n",
    "    m = SAFE_BLOCK_RE.search(generated)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    # fallback: keep everything after first <safe> if closing tag missing\n",
    "    if \"<safe>\" in generated:\n",
    "        return generated.split(\"<safe>\", 1)[1].strip()\n",
    "    return generated.strip()\n",
    "\n",
    "gen_v2 = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "def redact_infer_v2(text: str, max_new_tokens=96) -> str:\n",
    "    prompt = make_prompt_v2(text)\n",
    "    out = gen_v2(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,     # deterministic\n",
    "        top_p=1.0,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        return_full_text=False\n",
    "    )[0][\"generated_text\"]\n",
    "    # Hard stop at the closing tag if present\n",
    "    if \"</safe>\" in out:\n",
    "        out = out.split(\"</safe>\", 1)[0]\n",
    "    # Ensure we only return the inside of the block\n",
    "    return extract_safe(f\"<safe>{out}</safe>\")\n",
    "\n",
    "# Test again with the same example\n",
    "sample = ds[0][\"source_text\"]\n",
    "print(\"SOURCE:\", sample)\n",
    "masked_v2 = redact_infer_v2(sample)\n",
    "print(\"\\nPREDICTED:\")\n",
    "print(f\"<safe>{masked_v2}</safe>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80022340-4302-4964-946b-4aa30f02f90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313a5c527c8f45848530bb1f6287e038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0788d11a0b7421d844cc0fce7868b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting to chat template:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set', 'sft_text'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "\n",
      "Sample 1 (truncated):\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Oct 2025\n",
      "\n",
      "You redact personal or secret information from user text. Return the SAME text but with only the sensitive VALUES replaced by placeholders. Do not change surrounding words like 'IMEI', 'Email', 'Phone', or punctuation. Allowed placeholders include dataset labels such as [PHONEIMEI], [EMAIL], etc. Output ONLY the redacted text between <safe> and </safe>.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A student's assessment was found on device bearing IMEI: 06-184755-86\n",
      "\n",
      "Sample 2 (truncated):\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Oct 2025\n",
      "\n",
      "You redact personal or secret information from user text. Return the SAME text but with only the sensitive VALUES replaced by placeholders. Do not change surrounding words like 'IMEI', 'Email', 'Phone', or punctuation. Allowed placeholders include dataset labels such as [PHONEIMEI], [EMAIL], etc. Output ONLY the redacted text between <safe> and </safe>.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Dear Omer, as per our records, your license 78B5R2MVFAHJ48500 is stil\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3197171ce8174e9e86003f76c3dc0d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg tokens over 50 samples: 197.3 | Max: 258 | SEQ_LEN limit = 512\n"
     ]
    }
   ],
   "source": [
    "# Step 5 â€” Prepare promptâ†’answer strings for supervised fine-tuning (no training yet)\n",
    "from datasets import load_dataset\n",
    "\n",
    "# We reuse the tokenizer from earlier. If not present, re-create it quickly.\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    from transformers import AutoTokenizer\n",
    "    MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.model_max_length = 512  # keep in sync with your SEQ_LEN knob\n",
    "\n",
    "DATASET_ID = \"ai4privacy/pii-masking-200k\"\n",
    "\n",
    "# Load a manageable slice, then keep only English\n",
    "raw = load_dataset(DATASET_ID, split=\"train[:3000]\")\n",
    "raw_en = raw.filter(lambda x: (x.get(\"language\") or \"\").startswith(\"en\")).select(range(min(2000, len(raw))))\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You redact personal or secret information from user text. \"\n",
    "    \"Return the SAME text but with only the sensitive VALUES replaced by placeholders. \"\n",
    "    \"Do not change surrounding words like 'IMEI', 'Email', 'Phone', or punctuation. \"\n",
    "    \"Allowed placeholders include dataset labels such as [PHONEIMEI], [EMAIL], etc. \"\n",
    "    \"Output ONLY the redacted text between <safe> and </safe>.\"\n",
    ")\n",
    "\n",
    "def format_chat(input_text: str, target_masked: str) -> str:\n",
    "    # The assistant message includes the exact desired output between tags\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": input_text},\n",
    "        {\"role\": \"assistant\", \"content\": f\"<safe>{target_masked}</safe>\"},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "formatted = raw_en.map(\n",
    "    lambda ex: {\"sft_text\": format_chat(ex[\"source_text\"], ex[\"target_text\"])},\n",
    "    desc=\"Formatting to chat template\",\n",
    ")\n",
    "\n",
    "# Quick sanity: show two samples (truncated)\n",
    "print(formatted)\n",
    "print(\"\\nSample 1 (truncated):\\n\", formatted[0][\"sft_text\"][:600])\n",
    "print(\"\\nSample 2 (truncated):\\n\", formatted[1][\"sft_text\"][:600])\n",
    "\n",
    "# Token length check on a subset so we know how tight 512 is\n",
    "subset = formatted.select(range(min(50, len(formatted))))\n",
    "lens = subset.map(lambda ex: {\"len\": len(tokenizer(ex[\"sft_text\"], truncation=False)[\"input_ids\"])})\n",
    "avg_len = sum(r[\"len\"] for r in lens) / len(lens)\n",
    "max_len = max(r[\"len\"] for r in lens)\n",
    "print(f\"\\nAvg tokens over {len(lens)} samples: {avg_len:.1f} | Max: {max_len} | SEQ_LEN limit = {tokenizer.model_max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c23ca0e-6fe7-4b81-87ef-89d13a76b25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,175,040 || all params: 3,221,924,864 || trainable%: 0.2848\n",
      "VRAM allocated after LoRA wrap: 2.87 GB\n"
     ]
    }
   ],
   "source": [
    "# Step 6 â€” LoRA wrap (no training yet)\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Reuse knobs from earlier cells, or set safe defaults\n",
    "try:\n",
    "    LORA_R\n",
    "except NameError:\n",
    "    LORA_R = 16\n",
    "try:\n",
    "    LORA_ALPHA\n",
    "except NameError:\n",
    "    LORA_ALPHA = 32\n",
    "try:\n",
    "    LORA_DROPOUT\n",
    "except NameError:\n",
    "    LORA_DROPOUT = 0.05\n",
    "try:\n",
    "    TARGET_MODULES\n",
    "except NameError:\n",
    "    TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "assert \"base_model\" in globals(), \"Please run the model loading cell first to create `base_model`.\"\n",
    "\n",
    "# Make sure caching is off for checkpointing\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# If loaded in 4-bit/8-bit, prep for k-bit training, which also toggles grad checkpointing\n",
    "is_kbit = bool(getattr(base_model, \"is_loaded_in_4bit\", False) or getattr(base_model, \"is_loaded_in_8bit\", False))\n",
    "if is_kbit:\n",
    "    base_model = prepare_model_for_kbit_training(base_model, use_gradient_checkpointing=True)\n",
    "else:\n",
    "    # Fallback: still enable checkpointing\n",
    "    base_model.gradient_checkpointing_enable()\n",
    "\n",
    "# Define LoRA\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=TARGET_MODULES,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Wrap\n",
    "lora_model = get_peft_model(base_model, lora_cfg)\n",
    "\n",
    "# Report trainable params\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Quick VRAM check\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"VRAM allocated after LoRA wrap: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3affe10a-d6cc-40a4-907c-a509cbafb5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701fb758f2a740ad8f2574b00d976553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sft_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1800\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sft_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Step 7 â€” Tiny SFT using vanilla Trainer (labels only after \"<safe>\")\n",
    "import os, gc, torch\n",
    "from typing import List, Dict, Any\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import DatasetDict\n",
    "\n",
    "assert \"lora_model\" in globals(), \"LoRA model not found. Please run the LoRA wrap cell first.\"\n",
    "assert \"tokenizer\" in globals(), \"Tokenizer not found. Please load tokenizer.\"\n",
    "assert \"formatted\" in globals() and \"sft_text\" in formatted.column_names, \"Run the formatting step to create 'sft_text'.\"\n",
    "\n",
    "# 1) Find the token index of the response template\n",
    "RESP_TEMPLATE = \"<safe>\"\n",
    "resp_ids = tokenizer(RESP_TEMPLATE, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "def find_sublist(haystack: List[int], needle: List[int]) -> int:\n",
    "    \"\"\"Return start index of 'needle' in 'haystack', or -1 if not found.\"\"\"\n",
    "    if not needle or len(needle) > len(haystack):\n",
    "        return -1\n",
    "    # simple scan; fast enough for our batch sizes\n",
    "    for i in range(len(haystack) - len(needle) + 1):\n",
    "        if haystack[i:i+len(needle)] == needle:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "# 2) Tokenise and build labels with -100 before the response start\n",
    "def tok_and_mask(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    enc = tokenizer(\n",
    "        example[\"sft_text\"],\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    ids = enc[\"input_ids\"]\n",
    "    start = find_sublist(ids, resp_ids)\n",
    "    labels = [-100] * len(ids)\n",
    "    if start != -1:\n",
    "        # put loss on everything from the start of \"<safe>\" onwards\n",
    "        labels[start:] = ids[start:]\n",
    "    enc[\"labels\"] = labels\n",
    "    return enc\n",
    "\n",
    "tokenised = formatted.map(tok_and_mask, remove_columns=[c for c in formatted.column_names if c != \"sft_text\"])\n",
    "# Simple split\n",
    "splits = tokenised.train_test_split(test_size=0.1, seed=42)\n",
    "dsd = DatasetDict(train=splits[\"train\"], test=splits[\"test\"])\n",
    "print(dsd)\n",
    "\n",
    "# 3) Collator that pads input_ids, attention_mask, and labels together\n",
    "class CausalPadCollator(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        labels = [f.pop(\"labels\") for f in features]\n",
    "        batch = super().__call__(features)  # pads input_ids and attention_mask\n",
    "        # pad labels to same length with -100\n",
    "        max_len = batch[\"input_ids\"].shape[1]\n",
    "        padded = []\n",
    "        for lab in labels:\n",
    "            if len(lab) < max_len:\n",
    "                lab = lab + [-100] * (max_len - len(lab))\n",
    "            else:\n",
    "                lab = lab[:max_len]\n",
    "            padded.append(lab)\n",
    "        batch[\"labels\"] = torch.tensor(padded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "263cde8f-d5d0-4164-b76e-03d8493b96bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_364/2590458741.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trainingâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='113' max='113' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [113/113 34:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.581400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.286700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: {'eval_loss': nan, 'eval_runtime': 75.2314, 'eval_samples_per_second': 2.658, 'eval_steps_per_second': 2.658, 'epoch': 1.0}\n",
      "VRAM now: 2.94 GB\n",
      "Adapters saved to: ./outputs/safe-prompt-3b-lora\n"
     ]
    }
   ],
   "source": [
    "# Step 7c â€” Train for 1 epoch with minimal, version-safe TrainingArguments\n",
    "import os, gc, torch\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "assert \"dsd\" in globals(), \"The tokenised DatasetDict 'dsd' is missing. Please re-run the dataset formatting cell.\"\n",
    "assert \"lora_model\" in globals(), \"LoRA model not found. Please run the LoRA wrap cell.\"\n",
    "assert \"tokenizer\" in globals(), \"Tokenizer not found. Please load tokenizer.\"\n",
    "\n",
    "# Collator that pads input_ids, attention_mask, and labels together\n",
    "class CausalPadCollator(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        labels = [f.pop(\"labels\") for f in features]\n",
    "        batch = super().__call__(features)  # pads input_ids and attention_mask\n",
    "        # pad labels to same length with -100\n",
    "        max_len = batch[\"input_ids\"].shape[1]\n",
    "        padded = []\n",
    "        for lab in labels:\n",
    "            if len(lab) < max_len:\n",
    "                lab = lab + [-100] * (max_len - len(lab))\n",
    "            else:\n",
    "                lab = lab[:max_len]\n",
    "            padded.append(lab)\n",
    "        batch[\"labels\"] = torch.tensor(padded, dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "collator = CausalPadCollator(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "OUTPUT_DIR = \"./outputs/safe-prompt-3b-lora\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Keep args minimal for 4.57.1 compatibility (no evaluation_strategy here)\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",   # safest option with your stack\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=args,\n",
    "    train_dataset=dsd[\"train\"],\n",
    "    eval_dataset=dsd[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "print(\"Starting trainingâ€¦\")\n",
    "train_out = trainer.train()\n",
    "print(\"Training complete.\\n\")\n",
    "\n",
    "# Explicit evaluation (since we did not set an eval strategy)\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"Eval:\", eval_metrics)\n",
    "\n",
    "# Save adapters and tokenizer\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Cleanup\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"VRAM now: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(\"Adapters saved to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc468cbc-a998-4001-9a8e-4230f109a0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4c88528b2440949b47fd528b160d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SOURCE 1:\n",
      " A student's assessment was found on device bearing IMEI: 06-184755-866851-3. The document falls under the various topics discussed in our Optimization curriculum. Can you please collect it?\n",
      "\n",
      "PREDICTED:\n",
      "<safe><safe>A student's assessment was found on device bearing IMEI: [IMEI]. The document falls under the various topics discussed in our Optimization curriculum. Can you please collect it?</safe>\n",
      "\n",
      "SOURCE 2:\n",
      " Dear Omer, as per our records, your license 78B5R2MVFAHJ48500 is still registered in our records for access to the educational tools. Please feedback on it's operability.\n",
      "\n",
      "PREDICTED:\n",
      "<safe><safe>Dear [FIRSTNAME], as per our records, your license [VEHICLEVRM] is still registered in our records for access to the educational tools. Please feedback on it's operability.</safe>\n",
      "\n",
      "SOURCE 3:\n",
      " Kattie could you please share your recomndations about vegetarian diet for 72 old Intersex person with 158centimeters?\n",
      "\n",
      "PREDICTED:\n",
      "<safe><safe>Kattie could you please share your recomndations about vegetarian diet for [GENDER] person with [HEIGHT]?</safe>\n"
     ]
    }
   ],
   "source": [
    "# Step 8 â€” Reload adapters from disk and test inference with leak checks\n",
    "import os, re, gc, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# Knobs\n",
    "MODEL_ID   = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "ADAPTER_DIR = \"./outputs/safe-prompt-3b-lora\"\n",
    "SEQ_LEN    = 512\n",
    "DTYPE      = torch.float16\n",
    "\n",
    "# 1) Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.model_max_length = SEQ_LEN\n",
    "\n",
    "# 2) Load base model (4-bit if possible)\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "bnb_config = None\n",
    "ALLOW_4BIT = False\n",
    "try:\n",
    "    import bitsandbytes as bnb  # noqa\n",
    "    sm_major, _ = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
    "    ALLOW_4BIT = bool(torch.cuda.is_available() and sm_major >= 6)\n",
    "except Exception:\n",
    "    ALLOW_4BIT = False\n",
    "\n",
    "if ALLOW_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=DTYPE,\n",
    "    )\n",
    "\n",
    "kwargs = dict(\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "if bnb_config is not None:\n",
    "    kwargs[\"quantization_config\"] = bnb_config\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **kwargs)\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# 3) Attach trained adapters\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "model.eval()\n",
    "\n",
    "# 4) Build generator\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 5) Prompt and parser with strict tag-only contract\n",
    "SYSTEM = (\n",
    "    \"You redact personal or secret information from user text. \"\n",
    "    \"Return the SAME text but with only the sensitive VALUES replaced by placeholders. \"\n",
    "    \"Do NOT change surrounding words like 'IMEI', 'Email', 'Phone', labels, or punctuation. \"\n",
    "    \"Allowed placeholders include dataset-style tags like [EMAIL], [PHONEIMEI], etc. \"\n",
    "    \"Output ONLY the redacted text between <safe> and </safe>. No other text.\"\n",
    ")\n",
    "\n",
    "def make_prompt(text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "        {\"role\": \"assistant\", \"content\": \"<safe>\"},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def redact(text: str, max_new_tokens=128) -> str:\n",
    "    prompt = make_prompt(text)\n",
    "    out = gen(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,            # deterministic\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        return_full_text=False\n",
    "    )[0][\"generated_text\"]\n",
    "    # Trim at the first closing tag if present\n",
    "    if \"</safe>\" in out:\n",
    "        out = out.split(\"</safe>\", 1)[0]\n",
    "    return out.strip()\n",
    "\n",
    "# 6) Light leak checks with conservative fallback\n",
    "EMAIL_RE = re.compile(r\"\\b[^\\s@]+@[^\\s@]+\\.[^\\s@]+\\b\")\n",
    "PHONE_RE = re.compile(r\"\\b(?:\\+?\\d{1,3}[\\s.\\-]?)?(?:\\(?\\d{3}\\)?[\\s.\\-]?)?\\d{3}[\\s.\\-]?\\d{4}\\b\")\n",
    "\n",
    "def postcheck(output_text: str) -> str:\n",
    "    fixed = output_text\n",
    "    # Basic masks if the model leaked values\n",
    "    fixed = EMAIL_RE.sub(\"[EMAIL]\", fixed)\n",
    "    fixed = PHONE_RE.sub(\"[PHONE]\", fixed)\n",
    "    return fixed\n",
    "\n",
    "def redact_safe(text: str) -> str:\n",
    "    masked = redact(text)\n",
    "    masked = postcheck(masked)\n",
    "    return f\"<safe>{masked}</safe>\"\n",
    "\n",
    "# 7) Try a few samples (uses 'ds' from earlier; fallback to load one)\n",
    "try:\n",
    "    ds\n",
    "except NameError:\n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(\"ai4privacy/pii-masking-200k\", split=\"train[:3]\")\n",
    "\n",
    "for i in range(min(3, len(ds))):\n",
    "    src = ds[i][\"source_text\"]\n",
    "    print(f\"\\nSOURCE {i+1}:\\n\", src)\n",
    "    print(\"\\nPREDICTED:\")\n",
    "    print(redact_safe(src))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fe5b1bb-583d-43ac-8b41-e70e150e6159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT 1: Hi, I am Vishal Shinde. Email me at vishal@example.com and call +1 415 555 0199.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: Hi, I am [FIRSTNAME] [LASTNAME]. Email me at [EMAIL] and call [PHONEIMEI].\n",
      "\n",
      "INPUT 2: Card 4111 1111 1111 1111 expires 12/26, CVV 123.\n",
      "OUTPUT: Card [MASKEDNUMBER] [MASKEDNUMBER] [MASKEDNUMBER] [MASKEDNUMBER] expires [DATE], CVV [MASKEDNUMBER].\n",
      "\n",
      "INPUT 3: Server at 10.0.0.5 uses API key sk_live_ABC123XYZ456 for uploads.\n",
      "OUTPUT: Server at [IP] uses API key [PASSWORD] for uploads.\n",
      "\n",
      "INPUT 4: IMEI: 06-184755-866851-3 belongs to this device.\n",
      "OUTPUT: IMEI: [IMEI] belongs to this device.\n",
      "\n",
      "INPUT 5: My SSN is 123-45-6789. Do not store it.\n",
      "OUTPUT: My SSN is [SSN]. Do not store it.\n",
      "\n",
      "INPUT 6: Send the package to 221B Baker Street, London.\n",
      "OUTPUT: Send the package to [BUILDINGNUMBER] [STREET], [CITY].\n"
     ]
    }
   ],
   "source": [
    "# Step 9 â€” Redaction tester for arbitrary inputs\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Callable\n",
    "from transformers import pipeline\n",
    "\n",
    "SAFE_OPEN = \"<safe>\"\n",
    "SAFE_CLOSE = \"</safe>\"\n",
    "\n",
    "# Reuse tokenizer/model already in memory. If 'gen' is missing, rebuild it.\n",
    "try:\n",
    "    gen\n",
    "except NameError:\n",
    "    gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "SYSTEM_TEST = (\n",
    "    \"You redact personal or secret information from user text. \"\n",
    "    \"Return the SAME text but replace only sensitive VALUES with placeholders. \"\n",
    "    \"Do not change surrounding words like 'IMEI', 'Email', 'Phone', or punctuation. \"\n",
    "    \"Allowed placeholders include dataset-style tags like [EMAIL], [PHONEIMEI], [FIRSTNAME], etc. \"\n",
    "    \"Output ONLY the redacted text between <safe> and </safe>. No other text.\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class Detector:\n",
    "    name: str\n",
    "    pattern: re.Pattern\n",
    "    placeholder: str\n",
    "    post: Callable[[str], str] = lambda s: s\n",
    "    \n",
    "def make_prompt_v3(text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_TEST},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "        {\"role\": \"assistant\", \"content\": SAFE_OPEN},  # model will continue after <safe>\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def strip_one_prefix(s: str, prefix: str) -> str:\n",
    "    return s[len(prefix):] if s.startswith(prefix) else s\n",
    "\n",
    "def redact_raw(text: str, max_new_tokens: int = 128) -> str:\n",
    "    prompt = make_prompt_v3(text)\n",
    "    out = gen(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        return_full_text=False\n",
    "    )[0][\"generated_text\"]\n",
    "    # Cut at first closing tag if present\n",
    "    if SAFE_CLOSE in out:\n",
    "        out = out.split(SAFE_CLOSE, 1)[0]\n",
    "    # Remove any leading <safe> that the model may echo\n",
    "    out = strip_one_prefix(out.strip(), SAFE_OPEN)\n",
    "    # Final cleanup: if model still echoed tags inside, strip all\n",
    "    out = out.replace(SAFE_OPEN, \"\").replace(SAFE_CLOSE, \"\").strip()\n",
    "    return out\n",
    "\n",
    "# A tiny starter set; extend as needed\n",
    "DETECTORS: List[Detector] = [\n",
    "    Detector(\"email\", re.compile(r\"\\b[^\\s@]+@[^\\s@]+\\.[^\\s@]+\\b\"), \"[EMAIL]\"),\n",
    "    Detector(\"phone\", re.compile(r\"\\b(?:\\+?\\d{1,3}[\\s.\\-]?)?(?:\\(?\\d{3}\\)?[\\s.\\-]?)?\\d{3}[\\s.\\-]?\\d{4}\\b\"), \"[PHONE]\"),\n",
    "    # Detector(\"ipv4\", re.compile(r\"\\b(?:(?:25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\.){3}(?:25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\b\"), \"[IP]\"),\n",
    "    # Detector(\"ssn_us\", re.compile(r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\"), \"[SSN]\"),\n",
    "]\n",
    "\n",
    "def apply_validators(text: str, mode: str = \"warn\") -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    mode = \"off\" | \"warn\" | \"enforce\"\n",
    "    returns: (possibly_modified_text, list_of_detector_names_that_matched)\n",
    "    \"\"\"\n",
    "    hits = []\n",
    "    out = text\n",
    "    for d in DETECTORS:\n",
    "        if d.pattern.search(out):\n",
    "            hits.append(d.name)\n",
    "            if mode == \"enforce\":\n",
    "                out = d.pattern.sub(d.placeholder, out)\n",
    "    return out, hits\n",
    "    \n",
    "# Example integration with your redact function:\n",
    "def redact_safe_prompt(text: str, max_new_tokens: int = 128, validate_mode: str = \"enforce\") -> str:\n",
    "    masked = redact_raw(text, max_new_tokens=max_new_tokens)  # your LLM output inside the <safe> block\n",
    "    masked, hits = apply_validators(masked, mode=validate_mode)\n",
    "    # Optional: log hits to a counter or print in dev\n",
    "    # if hits: print(f\"[validator hits] {hits}\")\n",
    "    return masked\n",
    "\n",
    "# Try your own prompts by editing the list below\n",
    "tests = [\n",
    "    \"Hi, I am Vishal Shinde. Email me at vishal@example.com and call +1 415 555 0199.\",\n",
    "    \"Card 4111 1111 1111 1111 expires 12/26, CVV 123.\",\n",
    "    \"Server at 10.0.0.5 uses API key sk_live_ABC123XYZ456 for uploads.\",\n",
    "    \"IMEI: 06-184755-866851-3 belongs to this device.\",\n",
    "    \"My SSN is 123-45-6789. Do not store it.\",\n",
    "    \"Send the package to 221B Baker Street, London.\",\n",
    "]\n",
    "\n",
    "for i, t in enumerate(tests, 1):\n",
    "    print(f\"\\nINPUT {i}: {t}\")\n",
    "    print(\"OUTPUT:\", redact_safe_prompt(t))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
